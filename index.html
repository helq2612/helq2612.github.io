<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Liqiang He</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="keywords" content="Liqiang He, Personal Webpage, Ph.D. student, OSU, Oregon State, Sinisa Todorovic, Deep Learning, Computer Vision , US, Graduate student">
    <!-- <meta name="google-site-verification" content="ZsxjDJykOTsILWjMaY9zTQrHxx7fc5iV5g4y2GnLC3s" /> -->
    <link rel="stylesheet" href="assets/css/css/academicons.min.css" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">


</head>

<body class="is-preload">
    <!-- Header -->
    <header id="header">
        <div class="inner align-center">
            <a href="#" class="image avatar"><img src="images/Liqiang.JPG" alt="" /></a>
            <h1>
                <strong>Liqiang He</strong><br>
                <p><code id="email">E-mail: heli (AT) oregonstate.edu</code></p>
                <div class="inner align-center">
                    <br>
                    <code> <span id="cv">1-page CV</span>: </code>
                    <p><a class="align-center" href="./files/LiqiangHe_CV.pdf" id="heli"><i class="fa fa-file-download fa-2x"></i></a></p>
                    <br>
                </div>
            </h1>
        </div>
    </header> 
    
    <!-- Main -->
    <div id="main">


        <!-- ##################### INFO ############################# -->
        <section id="one">
            <header class="major">
                <!--
							<h2>Ipsum lorem dolor aliquam ante commodo<br />
							magna sed accumsan arcu neque.</h2>
-->
            </header>
            <div align="justify">
		    Hi, I recently completed my PhD final exam in Computer Science at the <a href="https://eecs.oregonstate.edu/">School of Electrical Engineering and Computer Science, Oregon State University</a>. 
		    I was advised by Prof. <a href="https://web.engr.oregonstate.edu/~sinisa/">Sinisa Todorovic</a>. 
		    My research primarily focused on Computer Vision, exploring areas such as Object Detection, Image and Video Semantic/Instance Segmentation, Object Tracking, and Cross-domain Learning. I am now seeking full-time opportunities where I can apply my expertise to real-world challenges.<br><br>
		    For more details about my work and to view my CV, please visit the rest of my site.
		</div>
            <!--
						<ul class="actions">
							<li><a href="#" class="button">Learn More</a></li>
						</ul>
-->
        </section>


        <!-- ##################### NEWS ############################# -->
        <section>
            <h2 class="align-center">News</h2>
            <br>

            <ul>
		    <li> <span class="dark">July 23:</span> Our paper "Bidirectional Alignment for Domain Adaptive Detection with Transformers" (BiADT), is accepted to the ICCV 2023! <a href="https://arxiv.org/pdf">[PDF]</a> <a href="https://github.com/helq2612/BiADT">[Github]</a>

            </ul>

        </section>



        <!-- ##################### PUBLICATIONS ############################# -->
        <section id="two">
            <h2 class="align-center">Publications</h2>
            <br>
            <div class="row">
                <ul class="">
                    
                    <!-- 2023 biADT -->
                    <div class="align-center">
                        <h4 class="dark">Bidirectional Alignment for Domain Adaptive Detection with Transformers
                            <span class="">(ICCV 23)</span></h4>
                            <h5><strong>Liqiang He</strong>, Wei Wang, Albert Chen, Min Sun, Cheng-hao Kuo, Sinisa Todorovic</h5>
                            <h5>[<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_Bidirectional_Alignment_for_Domain_Adaptive_Detection_with_Transformers_ICCV_2023_paper.pdf">PDF</a>] [<a href=" ">Code</a>] <a href="">[Poster]</a> </h5>
                                    </div>
                                    <br>
                                    <div class="box alt">
                                        <div class="row ">
                                            <!-- gtr-150 gtr-uniform -->
                                            <div class="col-4 col-12-small">
                                                <span class="image fit ">
                                                    <img src="images/papers/biADT1.png" alt="">
                                                </span>
                                            </div>
                                            <div class="col-8 col-12-small">
                                                <h5 align="justify">
                                                    We propose a Bidirectional Alignment for domain adaptive 
                                                    Detection with Transformers (BiADT) to improve cross
                                                    domain object detection performance. Existing adversarial
                                                    learning based methods use gradient reverse layer (GRL)
                                                    to reduce the domain gap between the source and target
                                                    domains in feature representations. Since different image
                                                    parts and objects may exhibit various degrees of domain-specific 
                                                    characteristics, directly applying GRL on a global
                                                    image or object representation may not be suitable. Our
                                                    proposed BiADT explicitly estimates token-wise domain-invariant and domain-specific features in the image and ob-
                                                    ject token sequences. BiADT has a novel deformable attention and self-attention, aimed at bi-directional domain
                                                    alignment and mutual information minimization. These two
                                                    objectives reduce the domain gap in domain-invariant representations, and simultaneously increase the distinctive-
                                                    ness of domain-specific features. Our experiments show
                                                    that BiADT achieves very competitive performance to SOTA
                                                    consistently on Cityscapes-to-FoggyCityscapes, Sim10K-to-Citiscapes and Cityscapes-to-BDD100K, outperforming the
                                                    strong baseline, AQT, by 1.9, 2.1, and 2.4 in mAP50, respectively.
                                                </h5>
                                            </div>
                                        </div>
                                    </div>

                    <hr>                    
                    <!-- 2023 agaid_trunk -->
                    <div class="align-center">
                        <h4 class="dark">Automatic estimation of trunk cross sectional area using deep learning
                            <span class="">(ECPA 23)</span></h4>
                            <h5><strong>T Wang, P Sankari, J Brown, A Paudel, L He</strong>, M Karkee, A Thompson, C Grimm, JR Davidson, S Todorovic</h5>
                            <h5>[<a href="https://web.engr.oregonstate.edu/~wangtie/files/ECPA23.pdf">PDF</a>] [<a href=" ">Code</a>] <a href="">[Poster]</a> </h5>
                                    </div>
                                    <br>
                                    <div class="box alt">
                                        <div class="row ">
                                            <!-- gtr-150 gtr-uniform -->
                                            <div class="col-4 col-12-small">
                                                <span class="image fit ">
                                                    <img src="images/papers/agaid_trunk_paper.png" alt="">
                                                </span>
                                            </div>
                                            <div class="col-8 col-12-small">
                                                <h5 align="justify">
                                                    This paper presents an automated method for estimating the trunk cross sectional area of
                                                    fruit trees. An Intel RealSense 435i was used to capture RGB images and point clouds of
                                                    individual trunks. To segment the trunk in the image from the background, a Maskedattention 
                                                    Mask Transformer model was adopted. The segmentation results were
                                                    integrated with the 3D point cloud to estimate trunk widths in 3D. The width estimation
                                                    was evaluated on three diverse datasets collected from a commercial apple orchard using
                                                    human measurements as ground truth. With a mean absolute error less than 5%, the
                                                    method is sufficiently accurate to assist orchard operations. 
                                                </h5>
                                            </div>
                                        </div>
                                    </div>
                    <hr>
                    <!-- 2022 destr -->
                    <div class="align-center">
                        <h4 class="dark">DESTR: Object Detection with Split Transformer
                            <span class="">(CVPR 22)</span></h4>
                            <h5><strong>Liqiang He</strong>, Sinisa Todorovic</h5>
                            <h5>[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_DESTR_Object_Detection_With_Split_Transformer_CVPR_2022_paper.pdf">PDF</a>] [<a href="https://github.com/helq2612/destr">Code</a>] <a href="files/cvpr22_destr_poster_05442.pdf">[Poster]</a> </h5>
                                    </div>
                                    <br>
                                    <div class="box alt">
                                        <div class="row ">
                                            <!-- gtr-150 gtr-uniform -->
                                            <div class="col-4 col-12-small">
                                                <span class="image fit ">
                                                    <img src="images/papers/destr_pipeline.png" alt="">
                                                </span>
                                            </div>
                                            <div class="col-8 col-12-small">
                                                <h5 align="justify">
                                                    Self- and cross-attention in Transformers provide for
                                                    high model capacity, making them viable models for object detection. 
                                                    However, Transformers still lag in performance behind CNN-based detectors. 
                                                    This is, we believe, because: (a) Cross-attention is used for both classification and bounding-box regression tasks; 
                                                    (b) Transformer's decoder poorly initializes content queries; and (c)
                                                    Self-attention poorly accounts for certain prior knowledge
                                                    which could help improve inductive bias. These limitations
                                                    are addressed with the corresponding three contributions.
                                                    First, we propose a new Detection Split Transformer (DESTR) that separates estimation of cross-attention into two
                                                    independent branches -- one tailored for classification and
                                                    the other for box regression. Second, we use a mini-detector
                                                    to initialize the content queries in the decoder with classification and regression embeddings of the respective heads
                                                    in the mini-detector. Third, we augment self-attention in the
                                                    decoder to additionally account for pairs of adjacent object
                                                    queries. Our experiments on the MS-COCO dataset show
                                                    that DESTR outperforms DETR and its successors.
                                                </h5>
                                            </div>
                                        </div>
                                    </div>

                    <hr>
                    <!-- 2022 peca -->
                    <div class="align-center">
                        <h4 class="dark">A polar-edge context-aware (PECA) network for mirror segmentation
                            <span class="">(Image and Vision Computing, 22)</span></h4>
                            <h5><strong>Liqiang He</strong>, Lu Xia, Jiajia Luo, Ke Zhang, Yuyin Sun, Nan Qiao, Cheng-Hao Kuo, Sinisa Todorovic</h5>
                            <h5>[<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-e4_WQUAAAAJ&citation_for_view=-e4_WQUAAAAJ:Y0pCki6q_DkC">PDF</a>] </a> </h5>
                                    </div>
                                    <br>
                                    <div class="box alt">
                                        <div class="row ">
                                            <!-- gtr-150 gtr-uniform -->
                                            <div class="col-4 col-12-small">
                                                <span class="image fit ">
                                                    <img src="images/papers/peca_pipeline.png" alt="">
                                                </span>
                                            </div>
                                            <div class="col-8 col-12-small">
                                                <h5 align="justify">
                                                    This paper presents Polar-Edge Contrast-Aware Network
                                                    (PECA) for mirror instance segmentation in images of indoor scenes. General instance segmentation methods typi-
                                                    cally rely on the surface appearance of the object to identify the foreground from background. However, these 
                                                    approaches do not directly apply to mirrors as the mirror surfaces are less reliable due to reflections of the surroundings.
                                                    On the other hand, the existing saliency-based mirror segmentation methods are prone to predict false positives in 
                                                    images with no mirrors, especially for the indoor scenes that
                                                    have mirror-shape objects, such as doors and windows. In
                                                    this work, we propose a novel boundary-based mirror localization method PECA that achieves both high segmentation
                                                    accuracy on mirrors and low false positive rate on negative
                                                    samples. PECA uses a context-aware module to extract features along the instance contour and in this way incorporates
                                                    boundary information for improving mirror detection. The
                                                    predicted mirror candidates are further refined with a local
                                                    contrast module for the final mirror instance segmentation.
                                                    PECA achieves IoU 80.29% on the benchmark Mirror Segmentation dataset (MSD), outperforming the state-of-the-art
                                                    method MirrorNet (IoU=78.95%) by 1.34%. It also produces
                                                    a significantly smaller false positive rate (43.37%) than existing methods (91.39%) on our challenging Negative Mir-
                                                    ror Dataset (NMD) without retraining. After training on both
                                                    MSD and NMD training sets, our model further reduces the
                                                    false positive rate to 0.08% on NMD testing set, while keeping IoU of 73.07% on MSD, enabling realistic real-world applications.
                                                </h5>
                                            </div>
                                        </div>
                                    </div>
                    <hr>
                    <!-- 2019 aaai_spec -->
                    
                    <li>
                        <h5 align="justify">Rebecca Hutchinson, <strong> Liqiang He </strong>, Sarah Emerson. "Species distribution modeling of citizen science data as a classification problem with class-conditional noise.", Proceedings of the AAAI Conference on Artificial Intelligence 31.
                            [<a href="./files/aaai_spec.pdf">PDF</a>]</h5>
                    </li>
                    
                </ul>
            </div>
        </section>
    </div>



    <!-- ########################### FOOTER ############################# -->

    <!-- Footer -->
    <footer id="footer">
        <div class="inner align-center">
            <br>
            <ul class="icons">
                <!--						<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>-->
                <li><a href="https://github.com/helq2612" class="icon tokhmi fa fa-github"><span class="label">Github</span></a></li>
                <li><a href="https://www.linkedin.com/in/liqianghe/" class="icon fa tokhmi fa-linkedin"><span class="label">LinkedIn</span></a></li>
                <li><a href="https://scholar.google.com/citations?user=-e4_WQUAAAAJ&hl=en" id="helq2612"><i class="icon ai ai-google-scholar"></i></a></li>
                <!--						<li><a href="#" class="icon fa-envelope-o"><span class="label">Email</span></a></li>-->
            </ul>
            <ul class="copyright">

                <li>&copy; 2021 Liqiang He </li>
                <li>Theme By: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.poptrox.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>
